---
layout: page
title: About
permalink: /about/
---

This website aims to provide a visual map of the conceptual shape of the field of existential AI safety. This site also doubles as my project for the March 2024 cohort of the [AI Safety Fundamentals Alignment Course](https://aisafetyfundamentals.com/alignment/) run by BlueDot Impact.

The concept map primarily came out of my difficulty wrapping my head around all the moving parts of the AI existential safety problem. Going through the [AI Safety Fundamentals Alignment Course](https://aisafetyfundamentals.com/alignment/) I was exposed to a lot of curated introductory material about AI alignment. It was obvious that the different approaches and considerations were highly interrelated, however, when reading through and discussion the materials it was difficult to keep track of how everything fit together into the big picture. Add to that the various online resources outlining a seemingly overwhelming number of ways things could go wrong (for example [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)) and I had a need to sit down to categorize and synthesize what I was learning.

The map is inspired by [AI Safety World](https://aisafety.world/) and is similar to visualizations such as [Mapping the Conceptual Territory in AI Existential Safety and Alignment](https://www.alignmentforum.org/posts/HEZgGBZTpT4Bov7nH/mapping-the-conceptual-territory-in-ai-existential-safety) while aiming to be a bit more concrete. It also draws principles from failure tree and bowtie diagrams to try and organize the concepts to show causal factors. The map is also focused more on existential risk so has less content about other important AI risks like surveillance, bias, and unemployment.

There is lots of future work to improve this map:
* Expand the number of concepts and themes represented. The map is currently heavily based on the content provided in the [AI Safety Fundamentals Alignment Course](https://aisafetyfundamentals.com/alignment/) run by BlueDot Impact. Next steps here are to add topics from the relevant Wikipedia pages and run the diagram past subject matter experts to get feedback on gaps or inaccuracies. 
* Move towards a more dynamic visualization. The current diagram is static without links which limits its ease of use. A first step would be to add links in a similar way to [AI Safety World](https://aisafety.world/), and to that end there is a [AI Safety Map Spreadsheet](https://docs.google.com/spreadsheets/d/1CFWHZQJPvF98DtyQtjiiK8upqksTPv-lAyKVYNSXCew/edit?usp=sharing) that has been created to facilitate this if you want to leave some suggestions! Other next steps here are to build a dashboard that allows the relationships between the different items to be displayed dynamically.

I hope you find the map clarifying and helpful!